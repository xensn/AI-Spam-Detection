{"cells":[{"cellId":"c0242c0a08b1419f9223492b52837e8b","cell_type":"code","metadata":{"cell_id":"c0242c0a08b1419f9223492b52837e8b","deepnote_cell_type":"code"},"source":"# Initialise Streamlit\nimport streamlit as st\nst.set_page_config(\n    page_title=\"AI Email Spam Detector\",\n    page_icon=\"üõ°Ô∏è\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)","block_group":"03b8c0d8bbfa45c1b5f83fdbd11873d5","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"f87828854e074345979d487906da98f8","cell_type":"code","metadata":{"cell_id":"f87828854e074345979d487906da98f8","deepnote_cell_type":"code"},"source":"# CSS Styling\nst.markdown(\"\"\"\n<style>\n    <style>\n    .main-header {\n    font-size: 3rem;\n    font-weight: bold;\n    text-align: center;\n    margin: 2rem 0;\n    padding: 1rem 0;\n    background: linear-gradient(135deg, #e53e3e 0%, #dd6b20 50%, #d69e2e 100%);\n    -webkit-background-clip: text;\n    -webkit-text-fill-color: transparent;\n    background-clip: text;\n    width: 100%;\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    }\n    \n    .prediction-box {\n        padding: 2rem;\n        border-radius: 15px;\n        text-align: center;\n        margin: 2rem 0;\n        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n    }\n    \n    .spam-prediction {\n        background: linear-gradient(135deg, #ff6b6b, #ee5a24);\n        color: white;\n    }\n    \n    .ham-prediction {\n        background: linear-gradient(135deg, #51cf66, #40c057);\n        color: white;\n    }\n    \n    .metric-card {\n        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n        color: white;\n        padding: 1rem;\n        border-radius: 10px;\n        text-align: center;\n        margin: 0.5rem;\n    }\n    \n    .feature-card {\n        background: #f8f9fa;\n        padding: 1.5rem;\n        border-radius: 10px;\n        border-left: 4px solid #1f77b4;\n        margin: 1rem 0;\n    }\n    \n    # Add this to your existing CSS in st.markdown at the top\n    .explanation-card {\n        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);\n        padding: 1rem;\n        border-radius: 10px;\n        border-left: 4px solid #007bff;\n        margin: 0.5rem 0;\n    }\n    \n    .spam-word {\n        background-color: #ffebee;\n        color: #c62828;\n        padding: 0.2rem 0.5rem;\n        border-radius: 15px;\n        margin: 0.1rem;\n        display: inline-block;\n    }\n    \n    .safe-word {\n        background-color: #e8f5e8;\n        color: #2e7d32;\n        padding: 0.2rem 0.5rem;\n        border-radius: 15px;\n        margin: 0.1rem;\n        display: inline-block;\n    }\n</style>\n\"\"\", unsafe_allow_html=True)","block_group":"ea476fbc128e42978e09c0ab8cbd6e01","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"34633debdb2543a69aea974838865e33","cell_type":"code","metadata":{"cell_id":"34633debdb2543a69aea974838865e33","deepnote_cell_type":"code"},"source":"# Imports\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nimport random\nimport os\nfrom pathlib import Path\nimport gc\nimport kagglehub\nfrom kagglehub import KaggleDatasetAdapter\nimport pandas as pd\npd.set_option('display.max_rows', None) #show all rows\npd.set_option('display.max_columns', None) #show all columns\nimport numpy as np\nimport math\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom datetime import datetime\nimport time\nimport re\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras.layers import Bidirectional, GlobalMaxPooling1D, Conv1D\nfrom keras.models import Model\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom plotly.subplots import make_subplots\nimport json\nfrom collections import Counter\nimport lime\nfrom lime.lime_text import LimeTextExplainer\nimport shap\nimport warnings\nwarnings.filterwarnings('ignore')\nimport io\nimport random\nfrom sklearn.metrics import roc_curve, auc","block_group":"0b257caa75214bd88de7948375649cbf","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"d5ddb934983949509abb29ed9e6c15f2","cell_type":"code","metadata":{"cell_id":"d5ddb934983949509abb29ed9e6c15f2","deepnote_cell_type":"code"},"source":"# Loading data from Kaggle (Stored in streamlit cache)\n@st.cache_resource\ndef load_data():\n    path = kagglehub.dataset_download(\"naserabdullahalam/phishing-email-dataset\")\n\n    print(\"Path to dataset files:\", path)\n    \n    file_paths = []\n    for roots, dirs, filenames in os.walk(path):\n        for filename in filenames:\n            file_paths.append(os.path.join(roots, filename))\n    files_to_remove = [\n    'phishing_email.csv',  # Combined file\n    ]\n\n    filtered_path = []\n    for file_path in file_paths:\n        filename = os.path.basename(file_path) # Get just the filename\n        if filename not in files_to_remove:\n            filtered_path.append(file_path)\n    \n    return filtered_path\n\n# Load Data\nfile_paths = load_data()","block_group":"8a7811afc7904f399b0b35cfe49c8f55","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"2c8dfc8dc25c42f3a41b8ad0c844107c","cell_type":"code","metadata":{"cell_id":"2c8dfc8dc25c42f3a41b8ad0c844107c","deepnote_cell_type":"code"},"source":"# Data pre-processing\n@st.cache_resource\ndef load_and_standardize_data(file_paths):\n    phishing_dfs = []\n\n    for file_path in file_paths:\n        try:\n            df_temp = pd.read_csv(file_path, low_memory=False)\n            print(f\"Original shape: {df_temp.shape}\")\n            print(f\"Original columns: {list(df_temp.columns)}\")\n\n            # Handle different file structures\n            if 'text_combined' in df_temp.columns:\n                # For the 2-column combined file\n                df_temp = df_temp[['text_combined', 'label']].copy()\n                df_temp.columns = ['Message', 'Category']\n            elif len(df_temp.columns) >= 2:\n                # For files with multiple columns, combine text fields\n                text_columns = []\n                if 'subject' in df_temp.columns:\n                    text_columns.append('subject')\n                if 'body' in df_temp.columns:\n                    text_columns.append('body')\n\n                if text_columns:\n                    # Combine subject and body\n                    df_temp['Message'] = df_temp[text_columns].fillna('').apply(\n                        lambda x: ' '.join(x.astype(str)), axis=1\n                    )\n                else:\n                    # Fallback to first text column\n                    df_temp['Message'] = df_temp.iloc[:, 0].astype(str)\n\n                # Get the label column\n                if 'label' in df_temp.columns:\n                    df_temp['Category'] = df_temp['label']\n                else:\n                    df_temp['Category'] = df_temp.iloc[:, -1]  # Assume last column is label\n\n                # Keep only the standardized columns\n                df_temp = df_temp[['Message', 'Category']].copy()\n\n            print(f\"Standardized shape: {df_temp.shape}\")\n            phishing_dfs.append(df_temp)\n\n        except Exception as e:\n            print(f\"Error loading {file_path}: {e}\")\n\n    return phishing_dfs\n\nphishing_dfs = load_and_standardize_data(file_paths)\ndf = pd.concat(phishing_dfs, ignore_index=True)","block_group":"4029264b3ae14c888be33617b0a6c7f2","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"b3045ca38eb04f40a3fdbfb031e86349","cell_type":"code","metadata":{"cell_id":"b3045ca38eb04f40a3fdbfb031e86349","deepnote_cell_type":"code"},"source":"# Data Cleaning\n@st.cache_resource\ndef clean_data(df):\n    # Check for empty columns\n    print(df.isnull().sum().sum())\n\n    # Check for duplicates & drop them\n    print(df.duplicated().sum())\n    df = df.drop_duplicates()\n    print(df.duplicated().sum())\n    \n    return df\n\n# Clean Data\ndf = clean_data(df)","block_group":"131b4cf3a1a74ee39977a7745257c731","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"3b44e90677654e4faec1d315895e9340","cell_type":"code","metadata":{"cell_id":"3b44e90677654e4faec1d315895e9340","deepnote_cell_type":"code"},"source":"# Advanced Text Preprocessing\ndef advanced_text_preprocessing(text):\n    \"\"\"Advanced text preprocessing with feature extraction\"\"\"\n    if pd.isna(text):\n        return \"\"\n    \n    text = str(text).lower()\n    \n    # Extract features before cleaning\n    features = {\n        'url_count': len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n                          , text)),\n        'exclamation_count': text.count('!'),\n        'question_count': text.count('?'),\n        'currency_symbols': len(re.findall(r'[$¬£‚Ç¨¬•‚Çπ]', text)),\n        'numbers': len(re.findall(r'\\d+', text)),\n        'special_chars': len(re.findall(r'[^\\w\\s]', text))\n    }\n    \n    # Clean text\n    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' URL '\n                    , text)\n    text = re.sub(r'[^\\w\\s]', ' ', text)\n    text = re.sub(r'\\d+', ' NUMBER ', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text, features\n","block_group":"5fda7519750346ac8bf96765f7c50ff9","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"4fa8447fd7024e2aac9f740489e9bdb3","cell_type":"code","metadata":{"cell_id":"4fa8447fd7024e2aac9f740489e9bdb3","deepnote_cell_type":"code"},"source":"# Model Training\n@st.cache_resource\ndef train_model(df):\n    # Prepare data for training\n    df = df.sample(n=30000, random_state=42).reset_index(drop=True)\n    processed_data = []\n    feature_data = []\n    \n    for text in df['Message']:\n        clean_text, features = advanced_text_preprocessing(text)\n        processed_data.append(clean_text)\n        feature_data.append(features)\n    \n    df['Processed_Message'] = processed_data\n    feature_df = pd.DataFrame(feature_data)\n    \n    X_text = df['Processed_Message']\n    X_features = feature_df.values\n    y = df['Category']\n\n    # Train-Validation-Test split\n    X_train, X_temp, X_feat_train, X_feat_temp, y_train, y_temp = train_test_split(\n        X_text, X_features, y, train_size=0.6, random_state=42, stratify=y\n    )\n    X_val, X_test, X_feat_val, X_feat_test, y_val, y_test = train_test_split(\n        X_temp, X_feat_temp, y_temp, train_size=0.5, random_state=42, stratify=y_temp\n    )\n\n    # Tokenization\n    # Translate text to numbers for model to understand\n    vocab_size = 20000  # Maximum number of words to keep\n    max_length = 200    # Maximum sequence length\n    oov_token = '<OOV>' # Out-of-vocabulary token (Handles unknown words that is not seen before)\n\n    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token) # Create a Dictionary to map words to numbers\n    tokenizer.fit_on_texts(X_train)  # Map words in training data to numbers \n\n    # Convert texts to sequences\n    X_train_sequences = tokenizer.texts_to_sequences(X_train) # Convert the training data words to numbers\n    X_test_sequences = tokenizer.texts_to_sequences(X_test) \n    X_val_sequences = tokenizer.texts_to_sequences(X_val)\n    \n    X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post')\n    X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post')\n    X_val_padded = pad_sequences(X_val_sequences, maxlen=max_length, padding='post')\n\n    # Convert y_val to numpy array if not already\n    y_val = np.array(y_val, dtype=np.int32)\n\n    # Clean text Input branch\n    text_input = tf.keras.Input(shape=(max_length,), name='text_input')\n    embedding = Embedding(vocab_size, 128, input_length=max_length, mask_zero=True)(text_input)\n    lstm = LSTM(64, dropout=0.6, recurrent_dropout=0.6)(embedding)\n    \n    # Feature Input branch\n    feature_input = tf.keras.Input(shape=(7,), name='feature_input')  # 7 features\n    feature_dense = Dense(32, activation='relu')(feature_input)\n    feature_dropout = Dropout(0.3)(feature_dense)\n    \n    # Combine branches\n    combined = tf.keras.layers.Concatenate()([lstm, feature_dropout])\n    dense1 = Dense(64, activation='relu')(combined)\n    dropout1 = Dropout(0.6)(dense1)\n    output = Dense(1, activation='sigmoid')(dropout1)\n\n    # Call Model\n    model = Model(inputs=[text_input, feature_input], outputs=output)\n    \n    # Compile model\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='binary_crossentropy', \n                  metrics=['accuracy'])\n\n    # Early Stopping - Keep track of val_loss and stops training if val_loss gets worse\n    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\n    history = model.fit([X_train_padded, X_feat_train], y_train, epochs=15, batch_size=64,\n                         validation_data=([X_val_padded, X_feat_val], y_val),callbacks=[early_stopping])\n\n    y_pred_prob = model.predict([X_test_padded,X_feat_test])\n    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n\n    # Overall Information of Model Prediction\n    test_loss, test_accuracy = model.evaluate([X_test_padded, X_feat_test], y_test, verbose=0)\n    test_precision = precision_score(y_test, y_pred)\n    test_recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    \n    feature_names = list(feature_df.columns)\n    \n    return {\n        'model': model,\n        'tokenizer': tokenizer,\n        'vocab_size': vocab_size,\n        'max_length': max_length,\n        'history': history,\n        'X_train': X_train,\n        'X_val': X_val,\n        'X_test': X_test,\n        'X_feat_train': X_feat_train,\n        'X_feat_test': X_feat_test,\n        'y_val': y_val,\n        'y_test': y_test,\n        'y_pred': y_pred,\n        'y_pred_prob': y_pred_prob,\n        'test_accuracy': test_accuracy,\n        'test_precision': test_precision,\n        'test_recall': test_recall,\n        'f1_score': f1,\n        'feature_names': feature_names,\n        'processed_df': df\n    }\n\n# Train the model\nmodel_data = train_model(df)","block_group":"3e4ef5ffdb0b47df9c7afcf37b1ad03b","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"cba9ba96c8764795b154e12d5d9cc916","cell_type":"code","metadata":{"cell_id":"cba9ba96c8764795b154e12d5d9cc916","deepnote_cell_type":"code"},"source":"# Unpack model data\nmodel = model_data['model']\ntokenizer = model_data['tokenizer']\nvocab_size = model_data['vocab_size']\nmax_length = model_data['max_length']\nhistory = model_data['history']\nX_train = model_data['X_train']\nX_val   = model_data['X_val']\nX_test  = model_data['X_test']\nX_feat_train = model_data['X_feat_train']\nX_feat_test = model_data['X_feat_test']\ny_val   = model_data['y_val']\ny_test  = model_data['y_test']\ny_pred = model_data['y_pred']\ny_pred_prob = model_data['y_pred_prob']\ny_pred_prob = model_data['y_pred_prob']\ntest_accuracy = model_data['test_accuracy']\ntest_precision = model_data['test_precision']\ntest_recall = model_data['test_recall']\nf1_score_val = model_data['f1_score']","block_group":"0d24846fd9ef40dfb2c821bd0afe4415","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"632e8d619b5046808edc60de1df18791","cell_type":"code","metadata":{"cell_id":"632e8d619b5046808edc60de1df18791","deepnote_cell_type":"code"},"source":"# Model Evaluation (Model Stats)\n@st.cache_resource\ndef report(y_true, y_pred):\n    # Print classification report\n    print(\"\\nClassification Report:\")\n    class_report = classification_report(y_test, y_pred)\n    \n    return class_report\n\n# Classification Report\nclass_report = report(y_test, y_pred)\nprint(class_report)\n\n@st.cache_resource\ndef confusion(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Plot Confusion Matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual') \n    plt.show()\n    \n    return cm\n\n# Confusion Matrix\ncm = confusion(y_test, y_pred)\n\ndef loss_validation(history):\n    # Plotting loss validation graph\n    history_df = pd.DataFrame(history.history)\n    history_df.loc[:, ['loss', 'val_loss']].plot()\n    print(\"Minimum validation loss: {:0.4f}\".format(history_df['val_loss'].min()))\n    \n    return history_df\n\n# Loss Validation Graph\nhistory_df = loss_validation(history)\n\n@st.cache_resource\ndef roc_auc_curve(y_test, y_pred_prob):\n    # Calculate ROC AUC\n    fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot ROC Curve\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    return fpr, tpr, roc_thresholds, roc_auc\n\n# ROC AUC Curve\nfpr, tpr, roc_thresholds, roc_auc = roc_auc_curve(y_test, y_pred_prob)","block_group":"730b27b4e4414480a40e4cd67bd4ff61","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"413bd2fd5e634df685adafaca5b68fcb","cell_type":"code","metadata":{"cell_id":"413bd2fd5e634df685adafaca5b68fcb","deepnote_cell_type":"code"},"source":"# Core Prediction Function\ndef predict_spam_message(message, threshold=0.5):\n    \"\"\"Enhanced prediction with feature extraction and confidence analysis\"\"\"\n    # Ensure message is not empt\n    try:\n        if not message or message.strip() == \"\":\n            return None\n        \n        # Start timer for performance measurement\n        start_time = time.time()\n        \n        # Advanced preprocessing\n        processed_message, features = advanced_text_preprocessing(message)\n        \n        # Tokenization\n        sequence = tokenizer.texts_to_sequences([processed_message])\n        padded = pad_sequences(sequence, maxlen=max_length, padding='post')\n        \n        # Feature array\n        feature_array = np.array([[\n            features['url_count'],\n            features['exclamation_count'], \n            features['question_count'],\n            features['currency_symbols'],\n            features['numbers'],\n            features['caps_ratio'],\n            features['special_chars']\n        ]])\n        \n        # Make prediction\n        prediction_prob = model.predict([padded, feature_array], verbose=0)[0][0]\n        \n        # Calculate results\n        predicted_class = 1 if prediction_prob > threshold else 0\n        confidence = prediction_prob if predicted_class == 1 else 1 - prediction_prob\n        \n        processing_time = time.time() - start_time\n        \n        # Risk analysis\n        risk_factors = []\n        if features['url_count'] > 2:\n            risk_factors.append(\"Multiple URLs detected\")\n        if features['exclamation_count'] > 3:\n            risk_factors.append(\"Excessive exclamation marks\")\n        if features['currency_symbols'] > 0:\n            risk_factors.append(\"Currency symbols present\")\n        if features['caps_ratio'] > 0.3:\n            risk_factors.append(\"High capital letter ratio\")\n        \n        # Add more risk factors based on your criteria\n        result = {\n            'prediction': 'Spam' if predicted_class == 1 else 'Ham',\n            'confidence': confidence,\n            'probability': prediction_prob,\n            'processing_time': processing_time,\n            'word_count': len(processed_message.split()),\n            'features': features,\n            'risk_factors': risk_factors,\n            'threat_level': 'High' if prediction_prob > 0.8 else 'Medium' if prediction_prob > 0.5 else 'Low'\n        }\n        \n        return result\n    # Handle any exceptions during prediction        \n    except Exception as e:\n        st.error(f\"Prediction error: {str(e)}\")\n        return None","block_group":"fbc067c1722b4f928d4416535b54511c","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"4ad4ae3f98ce486f875a1db9ba4fdbe4","cell_type":"code","metadata":{"cell_id":"4ad4ae3f98ce486f875a1db9ba4fdbe4","deepnote_cell_type":"code"},"source":"# Generate thought process of AI Decision Making using LIME to explain the prediction\ndef explain_prediction(message):\n    \"\"\"Generate user-friendly LIME explanation for prediction\"\"\"\n    try:\n        # Create LIME explainer with better configuration\n        def predictor_fn(texts):\n            processed_texts = []\n            features_list = []\n            for text in texts:\n                processed_text, features = advanced_text_preprocessing(text)\n                processed_texts.append(processed_text)\n                features_list.append([\n                    features['url_count'],\n                    features['exclamation_count'],\n                    features['question_count'],\n                    features['currency_symbols'],\n                    features['numbers'],\n                    features['caps_ratio'],\n                    features['special_chars']\n                ])\n            # Tokenize and pad the processed texts\n            sequences = tokenizer.texts_to_sequences(processed_texts)\n            padded = pad_sequences(sequences, maxlen=max_length, padding='post')\n            features_array = np.array(features_list, dtype=np.float32)\n            # Make predictions\n            predictions = model.predict([padded, features_array], verbose=0)\n            return np.column_stack([1-predictions, predictions])\n        \n        # Configure explainer for better word-level explanations\n        explainer = LimeTextExplainer(\n            class_names=['Safe Email', 'Spam Email'],\n            split_expression=r'\\W+',  # Split on non-word characters\n            bow=False  # Keep word order for better context\n        )\n        # Generate explanation\n        explanation = explainer.explain_instance(\n            message, \n            predictor_fn, \n            num_features=10,\n            num_samples=500,  # Increased for better accuracy\n            distance_metric='cosine'\n        )\n        return explanation\n    except Exception as e:\n        st.error(f\"Explanation error: {str(e)}\")\n        return None\n\n# Transform LIME Explaination into Readable Format\ndef create_user_friendly_explanation(explanation, prediction_result):\n    \"\"\"Convert LIME explanation to user-friendly format\"\"\"\n    if not explanation:\n        return None\n    \n    # Get explanation data\n    exp_data = explanation.as_list()\n    \n    # Create interpretable results\n    explanation_results = []\n    for word, impact in exp_data:\n        # Clean up the word/phrase\n        clean_word = word.strip().lower()\n        if len(clean_word) < 2:  # Skip very short words\n            continue\n            \n        # Determine impact direction and strength\n        if impact > 0:\n            direction = \"Increases SPAM likelihood\"\n            emoji = \"üö®\" if impact > 0.1 else \"‚ö†Ô∏è\" if impact > 0.05 else \"üìç\"\n            color = \"red\" if impact > 0.1 else \"orange\" if impact > 0.05 else \"blue\"\n        else:\n            direction = \"Indicates SAFE email\"\n            emoji = \"‚úÖ\" if impact < -0.1 else \"‚òëÔ∏è\" if impact < -0.05 else \"üìù\"\n            color = \"green\" if impact < -0.1 else \"lightgreen\" if impact < -0.05 else \"gray\"\n        \n        # Create user-friendly explanation\n        strength = \"Strong\" if abs(impact) > 0.1 else \"Moderate\" if abs(impact) > 0.05 else \"Weak\"\n        \n        # Append to results\n        explanation_results.append({\n            'word': clean_word,\n            'impact': impact,\n            'direction': direction,\n            'strength': strength,\n            'emoji': emoji,\n            'color': color,\n            'readable_impact': f\"{strength} indicator - {direction.lower()}\"\n        })\n    \n    return explanation_results","block_group":"36d9233d493247608ca422e764159a73","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"3fc2eb7504d843e9a224ad6501e14741","cell_type":"code","metadata":{"cell_id":"3fc2eb7504d843e9a224ad6501e14741","deepnote_cell_type":"code"},"source":"# Generate random email and its label\ndef get_random_test_email():\n    \"\"\"Get a random email for training game\"\"\"\n    # Predefined training emails for the game\n    training_emails = {\n        \"spam\": [\n            \"URGENT: Your account will be suspended! Click here immediately to verify: http://fake-bank.com\",\n            \"Congratulations! You've WON $1,000,000!!! Claim your prize NOW by clicking here!\",\n            \"FREE VIAGRA! No prescription needed! Order now with 90% discount!\",\n            \"ATTENTION: Your PayPal account has been limited. Restore access immediately!\",\n            \"Make $5000 per week working from home! No experience required! Click to start!\",\n            \"FINAL NOTICE: Your subscription will expire today! Renew now to avoid charges!\",\n            \"You have inherited $2,500,000 from a distant relative. Contact us to claim!\",\n            \"URGENT SECURITY ALERT: Suspicious activity detected. Verify your identity now!\"\n        ],\n        \"safe\": [\n            \"Meeting scheduled for tomorrow at 2 PM in conference room A. Please bring quarterly reports.\",\n            \"Thank you for your presentation yesterday. I'd like to schedule a follow-up meeting.\",\n            \"The research paper submission deadline has been extended to March 31st.\",\n            \"Please review the attached project proposal and provide your feedback by Friday.\",\n            \"Team lunch is scheduled for Thursday at 12:30 PM at the Italian restaurant downtown.\",\n            \"The software update will be deployed this weekend. Expect brief downtime on Sunday.\",\n            \"Your monthly newsletter: Latest updates from our development team and upcoming features.\",\n            \"Conference call notes from today's meeting are attached. Please review and confirm.\"\n        ]\n    }\n    \n    # Randomly select spam or safe, then pick random email from that category\n    category = random.choice(['spam', 'safe'])\n    email = random.choice(training_emails[category])\n    \n    return {\n        'content': email,\n        'true_label': category,\n        'category': category\n    }\n\n# Explain the reason behind the answers\ndef game_explaination(email_data, user_choice, ai_prediction):\n    \"\"\"Generate explanation for the training game\"\"\"\n    correct_label = email_data['true_label']\n    email_content = email_data['content']\n    \n    # Define explanations for spam and safe emails\n    explanations = {\n        'spam': {\n            'urgent_words': \"Contains urgent language like 'URGENT', 'IMMEDIATELY', 'NOW'\",\n            'money_offers': \"Promises unrealistic money or prizes\",\n            'suspicious_links': \"Contains suspicious links or requests to click\",\n            'poor_grammar': \"Has suspicious grammar or excessive punctuation\",\n            'impersonation': \"Pretends to be from legitimate institutions\",\n            'pressure_tactics': \"Uses pressure tactics to force quick action\"\n        },\n        'safe': {\n            'professional_tone': \"Uses professional, courteous language\",\n            'specific_details': \"Contains specific, verifiable information\",\n            'no_urgency': \"No artificial urgency or pressure tactics\",\n            'legitimate_sender': \"Appears to be from a legitimate, known source\",\n            'reasonable_request': \"Makes reasonable, work-related requests\",\n            'proper_grammar': \"Uses proper grammar and punctuation\"\n        }\n    }\n    \n    # Analyze the email content for specific indicators\n    email_lower = email_content.lower()\n    found_indicators = []\n    \n    # Check if the user's choice matches the AI's predictio\n    if correct_label == 'spam':\n        if any(word in email_lower for word in ['urgent', 'immediately', 'now', 'quick']):\n            found_indicators.append(explanations['spam']['urgent_words'])\n        if any(word in email_lower for word in ['won', 'prize', 'money', '$', 'free']):\n            found_indicators.append(explanations['spam']['money_offers'])\n        if 'http' in email_lower or 'click' in email_lower:\n            found_indicators.append(explanations['spam']['suspicious_links'])\n        if email_content.count('!') > 2:\n            found_indicators.append(explanations['spam']['poor_grammar'])\n    else:\n        if any(word in email_lower for word in ['meeting', 'schedule', 'please', 'thank']):\n            found_indicators.append(explanations['safe']['professional_tone'])\n        if any(word in email_lower for word in ['tomorrow', 'pm', 'conference', 'attached']):\n            found_indicators.append(explanations['safe']['specific_details'])\n    \n    # Check for urgency in safe emails\n    if not found_indicators:\n        found_indicators = [f\"General {correct_label} characteristics detected\"]\n    \n    return found_indicators","block_group":"06126ceee62a433aa46485aa87a6cbaa","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"eab142694bf44021a517f3e258f74b09","cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"eab142694bf44021a517f3e258f74b09","deepnote_cell_type":"text-cell-h1"},"source":"# Streamlit Application","block_group":"b724648e04644b059b5b5d9538b01f00"},{"cellId":"6096dc76dfe740d19f3354d987e9bc9c","cell_type":"code","metadata":{"cell_id":"6096dc76dfe740d19f3354d987e9bc9c","deepnote_cell_type":"code"},"source":"def main():\n    # Header\n    st.markdown('<h1 class=\"main-header\"> Have you been Phished? AI-Powered Email Spam Detector</h1>', unsafe_allow_html=True)\n    st.markdown('<p style=\"text-align: center; font-size: 1.2rem; color: #666;\">AI-Powered Email Spam Detector</p>', unsafe_allow_html=True)\n    \n    # Enhanced sidebar with real-time information\n    with st.sidebar:\n        st.header(\"Model Information\")\n        st.metric(\"Training Samples\", f\"{len(X_train):,}\")\n        st.metric(\"Vocabulary Size\", f\"{vocab_size:,}\")\n        st.metric(\"Max Sequence Length\", max_length)\n        st.metric(\"Model Accuracy\", f\"{test_accuracy:.1%}\")\n        st.metric(\"Precision\", f\"{test_precision:.3f}\")\n        st.metric(\"Recall\", f\"{test_recall:.3f}\")\n        st.metric(\"F1 Score\", f\"{f1_score_val:.3f}\")\n        st.metric(\"AUC Score\", f\"{auc_score * 100:.3f}\")\n        \n        st.header(\"‚öôÔ∏è Detection Settings\")\n        threshold = st.slider(\"Spam Threshold\", 0.1, 0.9, 0.5, 0.01)\n    \n    # Main content tabs\n    tab1, tab2, tab3, tab4 = st.tabs([\"Single Prediction\", \"Batch Testing\", \"Email Classification Game\", \"Model Stats\"])\n    \n    # TAB 1: Single Prediction\n    with tab1:\n        st.header(\"Advanced Email Analysis\")\n        \n        # Message input options\n        input_method = st.radio(\"Input Method:\", [\"Manual Input\", \"Example Messages\"])\n        \n        if input_method == \"Example Messages\":\n            example_messages = {\n                \"Phishing Email\": \"URGENT: Your account will be suspended unless you verify your identity immediately. Click here: http://fake-bank.com/verify\",\n                \"Nigerian Scam\": \"Congratulations! You have inherited $5,000,000 from a distant relative. Send $500 processing fee to claim your inheritance.\",\n                \"Legitimate Email\": \"Meeting scheduled for tomorrow at 2 PM in conference room A. Please bring your quarterly reports.\",\n                \"Marketing Spam\": \"LIMITED TIME OFFER! Get 90% OFF on premium products! Act now before it's too late! Click here to buy now!\",\n                \"Professional Email\": \"Thank you for your presentation yesterday. I'd like to schedule a follow-up meeting to discuss the project details.\"\n            }\n            \n            selected_example = st.selectbox(\"Choose an example:\", list(example_messages.keys()))\n            message = st.text_area(\n                \"Email content:\",\n                value=example_messages[selected_example],\n                height=150\n            )\n        else:\n            message = st.text_area(\n                \"Enter email content to analyze:\",\n                height=200,\n                placeholder=\"Paste your email content here...\\n\\nExample:\\n'Congratulations! You have won $1,000,000! Click here to claim your prize now!'\"\n            )\n        \n        # Toggle AI explanations\n        show_explanations = st.checkbox(\"Show AI Explanation\", value=False)\n        \n        if st.button(\"üîç Analyze Email\", key=\"single_predict\"):\n            if message.strip():\n                with st.spinner(\"Analyzing email with advanced AI model...\"):\n                    result = predict_spam_message(message, threshold=threshold)\n                \n                if result:\n                    # Enhanced prediction display\n                    prediction_class = \"spam-prediction\" if result['prediction'] == 'Spam' else \"ham-prediction\"\n                    threat_emoji = \"üö®\" if result['threat_level'] == 'High' else \"‚ö†Ô∏è\" if result['threat_level'] == 'Medium' else \"‚úÖ\"\n                    \n                    st.markdown(f\"\"\"\n                    <div class=\"prediction-box {prediction_class}\">\n                        <h2>{threat_emoji} Prediction: {result['prediction'].upper()}</h2>\n                        <h3>Confidence: {result['confidence']:.1%}</h3>\n                        <h4>Threat Level: {result['threat_level']}</h4>\n                    </div>\n                    \"\"\", unsafe_allow_html=True)\n                    \n                    # Detailed metrics\n                    col1, col2, col3, col4, col5 = st.columns(5)\n                    with col1:\n                        st.metric(\"Spam Probability\", f\"{result['probability']:.1%}\")\n                    with col2:\n                        st.metric(\"Processing Time\", f\"{result['processing_time']*1000:.1f}ms\")\n                    with col3:\n                        st.metric(\"Word Count\", result['word_count'])\n                    with col4:\n                        st.metric(\"URLs Found\", result['features']['url_count'])\n                    with col5:\n                        st.metric(\"Risk Factors\", len(result['risk_factors']))\n                    \n                    # Risk factors analysis\n                    if result['risk_factors']:\n                        st.subheader(\"Risk Factors Detected:\")\n                        for factor in result['risk_factors']:\n                            st.markdown(f\"‚Ä¢ {factor}\")\n                    \n                    # Probability gauge\n                    fig = go.Figure(go.Indicator(\n                        mode = \"gauge+number\",\n                        value = result['probability'] * 100,\n                        domain = {'x': [0, 1], 'y': [0, 1]},\n                        title = {'text': \"Spam Probability (%)\"},\n                        gauge = {\n                            'axis': {'range': [None, 100]},\n                            'bar': {'color': \"darkblue\"},\n                            'steps': [\n                                {'range': [0, 30], 'color': \"lightgreen\"},\n                                {'range': [30, 70], 'color': \"yellow\"},\n                                {'range': [70, 100], 'color': \"red\"}\n                            ],\n                            'threshold': {\n                                'line': {'color': \"red\", 'width': 4},\n                                'thickness': 0.75,\n                                'value': threshold * 100\n                            }\n                        }\n                    ))\n                    fig.update_layout(height=300)\n                    st.plotly_chart(fig, use_container_width=True)\n                    \n                    # AI Explanation\n                    if show_explanations:\n                        st.subheader(\"AI Model Explanation:\")\n                        st.info(\"**How to read this:** The AI analyzes individual words and phrases to make its decision. This shows which words influenced the classification and how.\")\n                        \n                        with st.spinner(\"Generating AI explanation...\"):\n                            explanation = explain_prediction(message)\n                            if explanation:\n                                # Get user-friendly explanation\n                                user_explanation = create_user_friendly_explanation(explanation, result)\n                                \n                                if user_explanation:\n                                    # Create two columns for better layout\n                                    exp_col1, exp_col2 = st.columns([2, 1])\n                                    \n                                    with exp_col1:\n                                        st.markdown(\"### AI Decision Summary\")\n                                        \n                                        # Show simple summary instead of complex graph\n                                        spam_indicators = [item for item in user_explanation if item['impact'] > 0]\n                                        safe_indicators = [item for item in user_explanation if item['impact'] < 0]\n                                        \n                                        if spam_indicators:\n                                            st.markdown(\"**Words that suggest SPAM:**\")\n                                            for item in spam_indicators[:5]:\n                                                impact_strength = \"Strong\" if abs(item['impact']) > 0.1 else \"Moderate\" if abs(item['impact']) > 0.05 else \"Weak\"\n                                                st.markdown(f\"‚Ä¢ **'{item['word']}'** - {impact_strength} spam signal\")\n                                        \n                                        if safe_indicators:\n                                            st.markdown(\"**Words that suggest SAFE email:**\")\n                                            for item in safe_indicators[:5]:\n                                                impact_strength = \"Strong\" if abs(item['impact']) > 0.1 else \"Moderate\" if abs(item['impact']) > 0.05 else \"Weak\"\n                                                st.markdown(f\"‚Ä¢ **'{item['word']}'** - {impact_strength} safety signal\")\n                                    \n                                    with exp_col2:\n                                        st.markdown(\"### Quick Overview\")\n                                        \n                                        total_spam_signals = len([item for item in user_explanation if item['impact'] > 0])\n                                        total_safe_signals = len([item for item in user_explanation if item['impact'] < 0])\n                                        strongest_signal = max(user_explanation, key=lambda x: abs(x['impact']))\n                                        \n                                        st.metric(\"Spam Signals Found\", total_spam_signals)\n                                        st.metric(\"Safe Signals Found\", total_safe_signals)\n                                        st.markdown(f\"**Strongest Signal:** '{strongest_signal['word']}' ({strongest_signal['strength']})\")\n                                    \n                                    # Detailed explanation table\n                                    st.markdown(\"### Detailed Word Analysis\")\n                                    \n                                    # Create readable table\n                                    exp_df = pd.DataFrame([{\n                                        'Word/Phrase': item['word'].title(),\n                                        'Effect on Decision': item['direction'],\n                                        'Signal Strength': item['strength'], \n                                        'Impact Score': f\"{item['impact']:.3f}\",\n                                        'What This Means': item['readable_impact']\n                                    } for item in user_explanation[:8]])\n                                    \n                                    st.dataframe(exp_df, use_container_width=True, hide_index=True)\n                                    \n                                    # Updated interpretation guide\n                                    st.markdown(\"\"\"\n                                    **Understanding the Table:**\n                                    \n                                    - **Word/Phrase**: The specific word or phrase the AI focused on\n                                    - **Effect on Decision**: Whether this word pushes toward SPAM or indicates a SAFE email\n                                    - **Signal Strength**: How much influence this word has (Strong/Moderate/Weak)\n                                    - **Impact Score**: Technical score (positive = more spam-like, negative = more safe)\n                                    - **What This Means**: Plain English explanation of the word's influence\n                                    \n                                    **Key Points:**\n                                    - Words with **higher impact scores** have more influence on the AI's decision\n                                    - **Positive scores** mean the word makes the email look more like spam\n                                    - **Negative scores** mean the word makes the email look more legitimate\n                                    - The AI considers **all words together** to make the final decision\n                                    \"\"\")\n                                else:\n                                    st.warning(\"Could not generate detailed explanation for this message.\")\n                    \n            else:\n                st.warning(\"Please enter an email message to analyze.\")\n    \n    # TAB 2: Batch Testing\n    with tab2:\n        st.header(\"Comprehensive Batch Testing\")\n    \n        # File upload options\n        st.subheader(\"Upload Your Email Data\")\n        \n        # Add .txt to file uploader\n        uploaded_file = st.file_uploader(\n            \"Upload file with emails\", \n            type=['txt'],\n            help=\"TXT files should have one email per line.\"\n        )\n        \n        # Reading TXT File\n        if uploaded_file is not None:\n            try:\n                file_extension = uploaded_file.name.split('.')[-1].lower()\n                \n                if file_extension == 'txt':\n                    # Handle .txt files\n                    st.info(\"Processing TXT file - each line will be treated as one email\")\n                    \n                    # Read the text file\n                    stringio = io.StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n                    file_content = stringio.read()\n                    \n                    # Split by lines and clean up\n                    email_lines = file_content.strip().split('\\n')\n                    email_lines = [line.strip() for line in email_lines if line.strip()]  # Remove empty lines\n                    \n                    # Create a list of emails\n                    batch_emails = email_lines\n                    st.success(f\"Loaded {len(batch_emails)} emails from TXT file\")\n                    \n                    # Show preview of first few emails\n                    with st.expander(\"Preview First 5 Emails\"):\n                        for i, email in enumerate(batch_emails[:5]):\n                            st.write(f\"**Email {i+1}:** {email[:100]}{'...' if len(email) > 100 else ''}\")\n                \n                # Analysis button\n                if st.button(\"Analyze All Emails\", key=\"analyze_uploaded\"):\n                    if len(batch_emails) > 1000:\n                        st.warning(f\"Large file detected ({len(batch_emails)} emails). This may take several minutes.\")\n                        proceed = st.checkbox(\"I understand this will take time. Proceed anyway.\")\n                        if not proceed:\n                            st.stop()\n                    \n                    # Progress tracking\n                    progress_bar = st.progress(0)\n                    status_text = st.empty()\n                    results = []\n                    \n                    # Process each email\n                    start_time = time.time()\n                    for i, email in enumerate(batch_emails):\n                        # Update progress\n                        progress = (i + 1) / len(batch_emails)\n                        progress_bar.progress(progress)\n                        status_text.text(f\"Processing email {i+1}/{len(batch_emails)} ({progress:.1%} complete)\")\n                        \n                        # Analyze email\n                        result = predict_spam_message(email, threshold=threshold)\n                        \n                        if result:\n                            results.append({\n                                'Email_ID': i + 1,\n                                'Message_Preview': email[:80] + \"...\" if len(email) > 80 else email,\n                                'Full_Message': email,  # Store full message for detailed view\n                                'Prediction': result['prediction'],\n                                'Confidence': f\"{result['confidence']:.1%}\",\n                                'Spam_Probability': result['probability'],\n                                'Threat_Level': result['threat_level'],\n                                'Risk_Factors_Count': len(result['risk_factors']),\n                                'Processing_Time_ms': f\"{result['processing_time']*1000:.1f}\",\n                                'Word_Count': result['word_count'],\n                                'URLs_Found': result['features']['url_count'],\n                                'Exclamation_Marks': result['features']['exclamation_count']\n                            })\n                    \n                    processing_time = time.time() - start_time\n                    progress_bar.progress(1.0)\n                    status_text.text(f\"Analysis complete! Processed {len(results)} emails in {processing_time:.1f} seconds\")\n                    \n                    # Store results in session state for later use\n                    st.session_state.batch_results = results\n                    st.session_state.batch_processing_time = processing_time\n                                    \n                    # Display result from session state\n                    if 'batch_results' in st.session_state and st.session_state.batch_results:\n                        results = st.session_state.batch_results\n                        processing_time = st.session_state.batch_processing_time\n                        \n                        st.success(f\"üéØ Analysis Complete! Processed {len(results)} emails in {processing_time:.1f} seconds\")\n                    \n                        # Display comprehensive results\n                        st.success(f\"Analysis Complete! Processed {len(results)} emails in {processing_time:.1f} seconds\")\n                        \n                        # Summary statistics\n                        spam_count = len([r for r in results if r['Prediction'] == 'Spam'])\n                        ham_count = len(results) - spam_count\n                        high_threat = len([r for r in results if r['Threat_Level'] == 'High'])\n                        avg_processing = sum([float(r['Processing_Time_ms']) for r in results]) / len(results)\n                        \n                        # Metrics display\n                        st.subheader(\"Analysis Summary\")\n                        metric_col1, metric_col2, metric_col3, metric_col4, metric_col5 = st.columns(5)\n                        \n                        with metric_col1:\n                            st.metric(\"Total Processed\", len(results))\n                        with metric_col2:\n                            st.metric(\"Spam Detected\", spam_count, f\"{(spam_count/len(results)*100):.1f}%\")\n                        with metric_col3:\n                            st.metric(\"Safe Emails\", ham_count, f\"{(ham_count/len(results)*100):.1f}%\")\n                        with metric_col4:\n                            st.metric(\"High Threat\", high_threat)\n                        with metric_col5:\n                            st.metric(\"Avg Time\", f\"{avg_processing:.1f}ms\")\n                        \n                        # Results table with better formatting\n                        st.subheader(\"Detailed Results\")\n                        \n                        # Create display dataframe (without full message for table view)\n                        display_df = pd.DataFrame([{\n                            'ID': r['Email_ID'],\n                            'Preview': r['Message_Preview'],\n                            'Classification': r['Prediction'],\n                            'Confidence': r['Confidence'],\n                            'Threat Level': r['Threat_Level'],\n                            'Risk Factors': r['Risk_Factors_Count'],\n                            'URLs': r['URLs_Found'],\n                            'Word Count': r['Word_Count']\n                        } for r in results])\n                        \n                        # Color-code the dataframe\n                        def color_rows(row):\n                            if row['Classification'] == 'Spam':\n                                return ['background-color: #ffebee; color: #000000; font-weight: bold'] * len(row)\n                            else:\n                                return ['background-color: #e8f5e8; color: #000000; font-weight: bold'] * len(row)\n\n                        \n                        styled_df = display_df.style.apply(color_rows, axis=1)\n                        st.dataframe(styled_df, use_container_width=True, hide_index=True)\n                        \n                        # Download enhanced results\n                        full_results_df = pd.DataFrame(results)\n                        csv = full_results_df.to_csv(index=False)\n                        st.download_button(\n                            label=\"Download Detailed Results (CSV)\",\n                            data=csv,\n                            file_name=f\"email_analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n                            mime=\"text/csv\",\n                            use_container_width=True\n                        )\n                        \n                        # Advanced visualizations\n                        st.subheader(\"Analysis Visualizations\")\n                        \n                        viz_col1, viz_col2 = st.columns(2)\n                        \n                        with viz_col1:\n                            # Pie chart\n                            fig_pie = px.pie(\n                                values=[spam_count, ham_count],\n                                names=['Spam', 'Safe'],\n                                title='Email Classification Distribution',\n                                color_discrete_map={'Spam': '#ff6b6b', 'Safe': '#51cf66'}\n                            )\n                            st.plotly_chart(fig_pie, use_container_width=True)\n                        \n                        with viz_col2:\n                            # Threat level distribution\n                            threat_counts = {}\n                            for result in results:\n                                threat_level = result['Threat_Level']\n                                threat_counts[threat_level] = threat_counts.get(threat_level, 0) + 1\n                            \n                            fig_threat = px.bar(\n                                x=list(threat_counts.keys()),\n                                y=list(threat_counts.values()),\n                                title='Threat Level Distribution',\n                                color=list(threat_counts.keys()),\n                                color_discrete_map={'High': '#ff4444', 'Medium': '#ffaa44', 'Low': '#44ff44'}\n                            )\n                            st.plotly_chart(fig_threat, use_container_width=True)\n                        \n                    if 'batch_results' in st.session_state:\n                        if st.button(\"Clear Results\", key=\"clear_batch_results\"):\n                            # Clear all batch-related session state\n                            if 'batch_results' in st.session_state:\n                                del st.session_state.batch_results\n                            if 'batch_processing_time' in st.session_state:\n                                del st.session_state.batch_processing_time\n                            if 'selected_email_id' in st.session_state:\n                                del st.session_state.selected_email_id\n                            st.rerun()\n                        \n            except Exception as e:\n                st.error(f\"Error processing file: {str(e)}\")\n                st.info(\"Make sure your TXT file has one email per line\")\n        \n        # Predefined test cases\n        st.subheader(\"Quick Test with Sample Messages\")\n        \n        test_categories = {\n            \"Phishing Emails\": [\n                \"URGENT: Your account will be suspended unless you verify your identity immediately. Click here: http://fake-bank.com/verify\",\n                \"Security Alert: Unusual activity detected on your account. Login here to secure: http://secure-login-fake.com\",\n                \"Your PayPal account has been limited. Verify your information to restore access: http://paypal-verify-fake.com\"\n            ],\n            \"Marketing Spam\": [\n                \"LIMITED TIME OFFER! Get 90% OFF on premium products! Act now before it's too late!\",\n                \"FREE VIAGRA! NO PRESCRIPTION NEEDED! ORDER NOW! DISCREET SHIPPING!\",\n                \"Make $5000 per week working from home! No experience required! Click here to start!\"\n            ],\n            \"Legitimate Emails\": [\n                \"Meeting scheduled for tomorrow at 2 PM in conference room A. Please bring your quarterly reports.\",\n                \"Thank you for your presentation yesterday. I'd like to schedule a follow-up meeting to discuss the project.\",\n                \"Research paper submission deadline extended to March 31st. Please submit via the university portal.\"\n            ]\n        }\n        \n        selected_category = st.selectbox(\"Choose test category:\", list(test_categories.keys()))\n        \n        if st.button(\"Test Sample Messages\"):\n            test_messages = test_categories[selected_category]\n            results = []\n            \n            progress_bar = st.progress(0)\n            for i, msg in enumerate(test_messages):\n                result = predict_spam_message(msg, threshold=threshold)\n                if result:\n                    results.append({\n                        'Message': msg[:80] + \"...\" if len(msg) > 80 else msg,\n                        'Prediction': result['prediction'],\n                        'Confidence': f\"{result['confidence']:.1%}\",\n                        'Probability': f\"{result['probability']:.3f}\",\n                        'Threat Level': result['threat_level'],\n                        'Processing Time': f\"{result['processing_time']*1000:.1f}ms\"\n                    })\n                progress_bar.progress((i + 1) / len(test_messages))\n            \n            # Display results\n            if results:\n                df_results = pd.DataFrame(results)\n                st.dataframe(df_results, use_container_width=True)\n                \n                # Summary\n                spam_count = len([r for r in results if r['Prediction'] == 'Spam'])\n                ham_count = len(results) - spam_count\n                \n                col1, col2 = st.columns(2)\n                with col1:\n                    st.metric(\"Spam Detected\", spam_count)\n                with col2:\n                    st.metric(\"Ham (Legitimate)\", ham_count)\n    \n    # TAB 3: AI Training Challenge\n    with tab3:\n        st.header(\"AI Training Challenge\")\n        st.markdown(\"\"\"\n        **Test your spam detection skills against our AI!**\n        \n        This interactive game helps you:\n        - Learn to identify spam patterns\n        - Compare your accuracy with AI\n        - Understand why emails are classified as spam or safe\n        - Improve your cybersecurity awareness\n        \"\"\")\n        \n        \"\"\"Interactive spam detection training\"\"\"\n        st.subheader(\"Spam Detection Training Game\")\n        st.markdown(\"**Challenge yourself!** Can you identify spam as well as our AI? Test your skills and learn!\")\n        \n        # Initialize game state\n        if 'game_score' not in st.session_state:\n            st.session_state.game_score = 0\n            st.session_state.game_round = 1\n            st.session_state.ai_score = 0\n            st.session_state.current_email = None\n            st.session_state.game_started = False\n        \n        # Game stats display\n        col1, col2, col3 = st.columns(3)\n        with col1:\n            st.metric(\"Round\", st.session_state.game_round)\n        with col2:\n            st.metric(\"Your Score\", st.session_state.game_score)\n        with col3:\n            st.metric(\"AI Score\", st.session_state.ai_score)\n        \n        # Start new round button\n        if st.button(\"Get New Email to Classify\") or not st.session_state.game_started:\n            st.session_state.current_email = get_random_test_email()\n            st.session_state.game_started = True\n            st.session_state.user_answered = False\n            st.rerun()\n        \n        # Display current email if available\n        if st.session_state.current_email:\n            st.markdown(\"### Classify this email:\")\n            \n            # Display the email in a nice box\n            st.code(st.session_state.current_email['content'], language=None)\n            \n            # User choice buttons\n            st.markdown(\"### What do you think?\")\n            col1, col2 = st.columns(2)\n            \n            user_choice = None\n            with col1:\n                if st.button(\"SAFE Email\", key=f\"safe_{st.session_state.game_round}\", use_container_width=True):\n                    user_choice = \"safe\"\n            with col2:\n                if st.button(\"SPAM Email\", key=f\"spam_{st.session_state.game_round}\", use_container_width=True):\n                    user_choice = \"spam\"\n            \n            # Process the answer\n            if user_choice and not getattr(st.session_state, 'user_answered', False):\n                st.session_state.user_answered = True\n                \n                # Get AI prediction\n                ai_prediction = predict_spam_message(st.session_state.current_email['content'])\n                ai_choice = \"spam\" if ai_prediction['prediction'].lower() == \"spam\" else \"safe\"\n                correct_answer = st.session_state.current_email['true_label']\n                \n                # Score both user and AI\n                user_correct = (user_choice == correct_answer)\n                ai_correct = (ai_choice == correct_answer)\n                \n                # Update scores\n                if user_correct:\n                    st.session_state.game_score += 1\n                if ai_correct:\n                    st.session_state.ai_score += 1\n                \n                # Display results\n                st.markdown(\"---\")\n                st.markdown(\"### Results:\")\n                \n                result_col1, result_col2, result_col3 = st.columns(3)\n                \n                with result_col1:\n                    if user_correct:\n                        st.success(f\"**You:** Correct!\\nYou said: {user_choice.upper()}\")\n                    else:\n                        st.error(f\"**You:** Wrong\\nYou said: {user_choice.upper()}\")\n                \n                with result_col2:\n                    if ai_correct:\n                        st.success(f\"**AI:** Correct!\\nAI said: {ai_choice.upper()} ({ai_prediction['confidence']:.1%} confidence)\")\n                    else:\n                        st.error(f\"**AI:** Wrong\\nAI said: {ai_choice.upper()} ({ai_prediction['confidence']:.1%} confidence)\")\n                \n                with result_col3:\n                    st.info(f\"**Correct Answer:** {correct_answer.upper()}\")\n                \n                # Show explanation\n                explanations = game_explaination(st.session_state.current_email, user_choice, ai_prediction)\n                \n                st.markdown(\"### üí° Why this email is \" + correct_answer.upper() + \":\")\n                for explanation in explanations:\n                    st.markdown(f\"‚Ä¢ {explanation}\")\n                \n                # Show detailed AI analysis if it was spam\n                if correct_answer == \"spam\":\n                    st.markdown(\"### üîç Detailed Threat Analysis:\")\n                    threat_factors = []\n                    content = st.session_state.current_email['content'].lower()\n                    \n                    if any(word in content for word in ['urgent', 'immediate', 'now', 'quick']):\n                        threat_factors.append(\"Urgency pressure tactics\")\n                    if any(word in content for word in ['click', 'http', 'link']):\n                        threat_factors.append(\"Suspicious links or click requests\")\n                    if any(word in content for word in ['$', 'money', 'prize', 'won', 'free']):\n                        threat_factors.append(\"Money/prize offers\")\n                    if st.session_state.current_email['content'].count('!') > 2:\n                        threat_factors.append(\"Excessive exclamation marks\")\n                    \n                    for factor in threat_factors:\n                        st.markdown(f\"‚Ä¢ {factor}\")\n                \n                # Prepare for next round\n                st.session_state.game_round += 1\n                \n                # Show progress\n                if st.session_state.game_round > 1:\n                    user_accuracy = (st.session_state.game_score / (st.session_state.game_round - 1)) * 100\n                    ai_accuracy = (st.session_state.ai_score / (st.session_state.game_round - 1)) * 100\n                    \n                    st.markdown(\"### Performance Comparison:\")\n                    \n                    perf_col1, perf_col2 = st.columns(2)\n                    with perf_col1:\n                        st.metric(\"Your Accuracy\", f\"{user_accuracy:.1f}%\")\n                    with perf_col2:\n                        st.metric(\"AI Accuracy\", f\"{ai_accuracy:.1f}%\")\n                    \n                    # Motivational messages\n                    if user_accuracy > ai_accuracy:\n                        st.success(\"You're beating the AI! Great spam detection skills!\")\n                    elif user_accuracy == ai_accuracy:\n                        st.info(\"You're tied with the AI! Impressive performance!\")\n                    else:\n                        st.warning(\"The AI is ahead, but keep trying! You're learning valuable skills!\")\n            \n            # Reset game option\n            if st.session_state.game_round > 5:\n                if st.button(\"üîÑ Start New Game\"):\n                    for key in ['game_score', 'game_round', 'ai_score', 'current_email', 'game_started', 'user_answered']:\n                        if key in st.session_state:\n                            del st.session_state[key]\n                    st.rerun()\n\n    # TAB 3: Model Statistics\n    with tab4:\n        st.header(\"üìä Model Performance & Statistics\")\n        \n        # Executive Summary Cards\n        st.subheader(\"üéØ Executive Summary\")\n        summary_col1, summary_col2, summary_col3, summary_col4 = st.columns(4)\n        \n        with summary_col1:\n            st.markdown(\"\"\"\n            <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n                    color: white; padding: 1.5rem; border-radius: 15px; text-align: center;\">\n                <h3 style=\"margin: 0; color: white;\">Model Accuracy</h3>\n                <h1 style=\"margin: 0.5rem 0; color: white;\">{:.1%}</h1>\n                <p style=\"margin: 0; color: white;\">Overall Performance</p>\n            </div>\n            \"\"\".format(test_accuracy), unsafe_allow_html=True)\n        \n        with summary_col2:\n            st.markdown(\"\"\"\n            <div style=\"background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); \n                    color: white; padding: 1.5rem; border-radius: 15px; text-align: center;\">\n                <h3 style=\"margin: 0; color: white;\">AUC Score</h3>\n                <h1 style=\"margin: 0.5rem 0; color: white;\">{:.3f}</h1>\n                <p style=\"margin: 0; color: white;\">ROC Performance</p>\n            </div>\n            \"\"\".format(auc_score), unsafe_allow_html=True)\n        \n        with summary_col3:\n            st.markdown(\"\"\"\n            <div style=\"background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); \n                    color: white; padding: 1.5rem; border-radius: 15px; text-align: center;\">\n                <h3 style=\"margin: 0; color: white;\">F1 Score</h3>\n                <h1 style=\"margin: 0.5rem 0; color: white;\">{:.3f}</h1>\n                <p style=\"margin: 0; color: white;\">Balanced Performance</p>\n            </div>\n            \"\"\".format(f1_score_val), unsafe_allow_html=True)\n        \n        with summary_col4:\n            st.markdown(\"\"\"\n            <div style=\"background: linear-gradient(135deg, #2d5a87 0%, #1e3c72 100%); \n                    color: white; padding: 1.5rem; border-radius: 15px; text-align: center;\">\n                <h3 style=\"margin: 0; color: white;\">Dataset Size</h3>\n                <h1 style=\"margin: 0.5rem 0; color: white;\">{:,}</h1>\n                <p style=\"margin: 0; color: white;\">Training Samples</p>\n            </div>\n            \"\"\".format(len(df)), unsafe_allow_html=True)\n        \n        st.markdown(\"---\")\n        \n        # Detailed Performance Metrics\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.subheader(\"üìà Model Configuration\")\n            config_data = {\n                \"Parameter\": [\n                    \"Model Type\", \"Total Dataset Size\", \"Training Set\", \"Validation Set\", \n                    \"Test Set\", \"Vocabulary Size\", \"Embedding Dimension\", \"LSTM Units\",\n                    \"Max Sequence Length\", \"Dropout Rate\", \"Learning Rate\", \"Early Stopping Patience\"\n                ],\n                \"Value\": [\n                    \"Hybrid LSTM + Features\", f\"{len(df):,} emails\", f\"{len(X_train):,} emails\",\n                    f\"{len(X_val):,} emails\", f\"{len(X_test):,} emails\", f\"{len(tokenizer.word_index):,} words\",\n                    \"128\", \"64\", str(max_length), \"0.6\", \"0.0005\", \"3 epochs\"\n                ]\n            }\n            config_df = pd.DataFrame(config_data)\n            st.dataframe(config_df, use_container_width=True, hide_index=True)\n            \n        with col2:\n            st.subheader(\"üéØ Performance Metrics\")\n            metrics_data = {\n                \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC Score\"],\n                \"Score\": [f\"{test_accuracy:.3f}\", f\"{test_precision:.3f}\", f\"{test_recall:.3f}\", \n                        f\"{f1_score_val:.3f}\", f\"{auc_score:.3f}\"],\n                \"Percentage\": [f\"{test_accuracy:.1%}\", f\"{test_precision:.1%}\", f\"{test_recall:.1%}\",\n                            f\"{f1_score_val:.1%}\", f\"{auc_score:.1%}\"]\n            }\n            metrics_df = pd.DataFrame(metrics_data)\n            st.dataframe(metrics_df, use_container_width=True, hide_index=True)\n        \n        st.markdown(\"---\")\n        \n        # Training History Analysis\n        st.subheader(\"üìä Training History Analysis\")\n        \n        # Process history data\n        history_df = pd.DataFrame(history.history)\n        min_val_loss = history_df['val_loss'].min()\n        final_epoch = len(history_df)\n        \n        # Training metrics\n        train_col1, train_col2, train_col3, train_col4 = st.columns(4)\n        with train_col1:\n            st.metric(\"Training Epochs\", final_epoch)\n        with train_col2:\n            st.metric(\"Min Validation Loss\", f\"{min_val_loss:.4f}\")\n        with train_col3:\n            st.metric(\"Final Training Loss\", f\"{history_df['loss'].iloc[-1]:.4f}\")\n        with train_col4:\n            early_stopped = \"Yes\" if final_epoch < 15 else \"No\"\n            st.metric(\"Early Stopping\", early_stopped)\n        \n        # Training vs Validation Loss Plot\n        fig_training = go.Figure()\n        \n        fig_training.add_trace(go.Scatter(\n            x=list(range(len(history_df))),\n            y=history_df['loss'],\n            mode='lines+markers',\n            name='Training Loss',\n            line=dict(color='#1f77b4', width=3),\n            marker=dict(size=6)\n        ))\n        \n        fig_training.add_trace(go.Scatter(\n            x=list(range(len(history_df))),\n            y=history_df['val_loss'],\n            mode='lines+markers',\n            name='Validation Loss',\n            line=dict(color='#ff7f0e', width=3),\n            marker=dict(size=6)\n        ))\n        \n        fig_training.update_layout(\n            title='Training vs Validation Loss Over Epochs',\n            xaxis_title='Epoch',\n            yaxis_title='Loss',\n            height=400,\n            template='plotly_white',\n            hovermode='x unified'\n        )\n        \n        st.plotly_chart(fig_training, use_container_width=True)\n        \n        st.markdown(\"---\")\n        \n        # ROC Curve and Confusion Matrix\n        st.subheader(\"üéØ Model Evaluation Visualizations\")\n        \n        viz_col1, viz_col2 = st.columns(2)\n        \n        with viz_col1:\n            st.markdown(\"#### ROC Curve Analysis\")\n            \n            # ROC Curve\n            fig_roc = go.Figure()\n            \n            # Add ROC curve\n            fig_roc.add_trace(go.Scatter(\n                x=fpr, y=tpr,\n                mode='lines',\n                name=f'ROC Curve (AUC = {roc_auc:.3f})',\n                line=dict(color='darkorange', width=3)\n            ))\n            \n            # Add diagonal reference line\n            fig_roc.add_trace(go.Scatter(\n                x=[0, 1], y=[0, 1],\n                mode='lines',\n                name='Random Classifier',\n                line=dict(color='navy', width=2, dash='dash')\n            ))\n            \n            fig_roc.update_layout(\n                title='ROC Curve',\n                xaxis_title='False Positive Rate',\n                yaxis_title='True Positive Rate',\n                height=400,\n                template='plotly_white',\n                showlegend=True\n            )\n            \n            st.plotly_chart(fig_roc, use_container_width=True)\n            \n            # ROC Interpretation\n            if roc_auc > 0.9:\n                roc_interpretation = \"üü¢ Excellent\"\n            elif roc_auc > 0.8:\n                roc_interpretation = \"üü° Good\"\n            elif roc_auc > 0.7:\n                roc_interpretation = \"üü† Fair\"\n            else:\n                roc_interpretation = \"üî¥ Poor\"\n            \n            st.markdown(f\"**AUC Interpretation:** {roc_interpretation} ({roc_auc:.3f})\")\n        \n        with viz_col2:\n            st.markdown(\"#### Confusion Matrix\")\n            \n            # Create confusion matrix heatmap\n            fig_cm = go.Figure(data=go.Heatmap(\n                z=cm,\n                x=['Predicted Ham', 'Predicted Spam'],\n                y=['Actual Ham', 'Actual Spam'],\n                colorscale='Blues',\n                text=cm,\n                texttemplate=\"%{text}\",\n                textfont={\"size\": 16},\n                hoverongaps=False\n            ))\n            \n            fig_cm.update_layout(\n                title='Confusion Matrix',\n                height=400,\n                template='plotly_white'\n            )\n            \n            st.plotly_chart(fig_cm, use_container_width=True)\n            \n            # Confusion Matrix Insights\n            tn, fp, fn, tp = cm.ravel()\n            st.markdown(f\"\"\"\n            **Matrix Breakdown:**\n            - True Negatives (Ham correctly identified): {tn:,}\n            - False Positives (Ham misclassified as Spam): {fp:,}\n            - False Negatives (Spam misclassified as Ham): {fn:,}\n            - True Positives (Spam correctly identified): {tp:,}\n            \"\"\")\n        \n        st.markdown(\"---\")\n        \n        # Detailed Classification Report\n        st.subheader(\"üìã Detailed Classification Report\")\n        \n        # Create a nicer classification report display\n        from sklearn.metrics import classification_report\n        report_dict = classification_report(y_test, y_pred, output_dict=True)\n        \n        # Extract metrics for each class\n        report_data = []\n        for class_name, metrics in report_dict.items():\n            if class_name in ['0', '1']:  # Only for actual classes\n                class_label = 'Ham (Safe)' if class_name == '0' else 'Spam'\n                report_data.append({\n                    'Class': class_label,\n                    'Precision': f\"{metrics['precision']:.3f}\",\n                    'Recall': f\"{metrics['recall']:.3f}\",\n                    'F1-Score': f\"{metrics['f1-score']:.3f}\",\n                    'Support': f\"{int(metrics['support']):,}\"\n                })\n        \n        # Add overall metrics\n        report_data.append({\n            'Class': 'Overall (Weighted)',\n            'Precision': f\"{report_dict['weighted avg']['precision']:.3f}\",\n            'Recall': f\"{report_dict['weighted avg']['recall']:.3f}\",\n            'F1-Score': f\"{report_dict['weighted avg']['f1-score']:.3f}\",\n            'Support': f\"{len(y_test):,}\"\n        })\n        \n        report_df = pd.DataFrame(report_data)\n        st.dataframe(report_df, use_container_width=True, hide_index=True)\n        \n        st.markdown(\"---\")\n        \n        # Performance Insights\n        st.subheader(\"üí° Model Performance Insights\")\n        \n        insight_col1, insight_col2 = st.columns(2)\n        \n        with insight_col1:\n            st.markdown(\"#### ‚úÖ **Strengths**\")\n            strengths = []\n            if test_accuracy > 0.9:\n                strengths.append(\"High overall accuracy (>90%)\")\n            if test_precision > 0.9:\n                strengths.append(\"Excellent precision - low false positives\")\n            if test_recall > 0.9:\n                strengths.append(\"Excellent recall - catches most spam\")\n            if auc_score > 0.9:\n                strengths.append(\"Outstanding ROC AUC performance\")\n            if len(strengths) == 0:\n                strengths.append(\"Model shows reasonable performance\")\n            \n            for strength in strengths:\n                st.markdown(f\"‚Ä¢ {strength}\")\n        \n        with insight_col2:\n            st.markdown(\"#### ‚ö†Ô∏è **Areas for Improvement**\")\n            improvements = []\n            if test_accuracy < 0.85:\n                improvements.append(\"Overall accuracy could be improved\")\n            if test_precision < 0.85:\n                improvements.append(\"Reduce false positive rate\")\n            if test_recall < 0.85:\n                improvements.append(\"Improve spam detection rate\")\n            if len(improvements) == 0:\n                improvements.append(\"Model performance is excellent!\")\n            \n            for improvement in improvements:\n                st.markdown(f\"‚Ä¢ {improvement}\")","block_group":"d8be5e0facbe40f8abe72cecfa425299","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cellId":"ec8d58f6adad4991bd7193ad279bdd4b","cell_type":"code","metadata":{"cell_id":"ec8d58f6adad4991bd7193ad279bdd4b","deepnote_cell_type":"code"},"source":"# Run the Streamlit app\nif __name__ == \"__main__\":\n    main()","block_group":"e93906a225ff4694ac9a3c2c9f49bded","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null}],
        "metadata": {"deepnote_app_layout":"powerful-article","deepnote_app_table_of_contents_enabled":false,"deepnote_app_hide_all_code_blocks_enabled":false,"deepnote_app_reactivity_enabled":true,"deepnote_notebook_id":"1c3a84b6082f4e59bd463e63c4a806c5"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }